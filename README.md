
# rlx: The "LangChain for Reinforcement Learning" (Alpha v0.3.0)

Hey everyone, Bereket here.

This is a project I'm building out of a desire for better tooling. As I've been learning AI and Reinforcement Learning, I realized that existing libraries (like `stable-baselines3`) are incredible, but they often feel like "black boxes." They are great for getting results, but hard to "tinker" with or hack.

I wanted something modularâ€”a "box of LEGOs" for RL, just like **LangChain** is for LLMs.

That's `rlx`.

-----

## ðŸŽ¯ The Goal: A "Glass Box" Framework

My goal is to create an open-source framework where every part of the RL pipelineâ€”the brain, the memory, the algorithmâ€”is a separate, interchangeable block.

We are moving from hard-coded scripts to declarative pipelines:

```python
from rlx.agents import PPOAgent
from rlx.train import Train
from rlx.env import EnvManager
from rlx.networks.vision import NatureCNN

# 1. Create a Vision Environment
env = EnvManager("CarRacing-v3")

# 2. Plug in a Vision Brain (The "LEGO" moment)
# We swap the default brain for a CNN just by passing an object.
cnn_policy = NatureCNN(input_channels=3, action_dim=env.action_space.shape[0])
agent = PPOAgent(env=env, policy=cnn_policy)

# 3. Train
trainer = Train(agent, env)
trainer.run(total_timesteps=1_000_000)
```

-----

## ðŸš€ Major Update: Vision Support\! (v0.3.0)

We have successfully built the "Holy Trinity" of RL inputs. The library now supports:

1.  **âœ… Discrete Control:** (e.g., `CartPole-v1`) - Pressing buttons.
2.  **âœ… Continuous Control:** (e.g., `Pendulum-v1`) - Turning knobs and steering wheels.
3.  **âœ… Visual Control:** (e.g., `CarRacing-v3`) - Learning from raw pixels using CNNs.

### The "Universal Adaptor" ðŸ¤–

The `PPOAgent` is now smart.

  * If you pass a **Discrete** environment, it builds a categorical (Softmax) brain.
  * If you pass a **Continuous** environment, it builds a Gaussian (Normal Distribution) brain.
  * If you pass a **Custom Policy** (like our new `NatureCNN`), it just works.

-----

## ðŸ§± The Architecture

Here is how the system is currently built:

  * **`rlx/agents/ppo.py`:** The "Surgeon." Manages the learning loop and handles the "Plug-and-Play" brain logic.
  * **`rlx/env/manager.py`:** Wraps Gymnasium environments and standardizes inputs.
  * **`rlx/train/trainer.py`:** The "Driver." Includes a real-time CLI dashboard for tracking rewards and losses.
  * **`rlx/networks/`**:
      * **`core.py`:** Standard MLP brains for Discrete & Continuous tasks.
      * **`vision.py`:** [NEW] The **NatureCNN** architecture for processing images.
  * **`rlx/utils/buffer.py`:** The "Memory." Handles both scalar (button) and vector (steering) storage.

-----

## ðŸ“‚ Project Structure

```text
rlx/
â”œâ”€â”€ rlx/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ base_agent.py
â”‚   â”‚   â””â”€â”€ ppo.py             # Core Algorithm
â”‚   â”œâ”€â”€ env/
â”‚   â”‚   â””â”€â”€ manager.py         # Gym Wrapper
â”‚   â”œâ”€â”€ networks/              
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ core.py            # MLP Brains
â”‚   â”‚   â””â”€â”€ vision.py          # CNN Eyes (The new update)
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â””â”€â”€ trainer.py         # Dashboard & Loop
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ buffer.py          # Memory
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ train_cartpole.py      # Demo: Discrete
â”‚   â”œâ”€â”€ train_pendulum.py      # Demo: Continuous
â”‚   â””â”€â”€ train_carracing.py     # Demo: Vision
â”œâ”€â”€ README.md
â””â”€â”€ pyproject.toml
```

-----

## ðŸ”® What's Next?

Now that the core engine works for all data types, the next phase is **Persistence & usability.**

  * **Saving/Loading Models:** We need to save the "Self-Driving Car" brain so we can watch it drive later\!
  * **Evaluation Mode:** A script to watch the agent play without training.

Stay tuned.

â€” Bereket

-----

